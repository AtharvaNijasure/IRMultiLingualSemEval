{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:38:07.893223Z",
     "start_time": "2024-11-22T20:38:07.878460Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           claim  \\\n",
      "fact_check_id                                                      \n",
      "0              ( Are avocados good for you?,  Are avocados go...   \n",
      "1              ( Can animals have headaches?,  Can animals ha...   \n",
      "2              ( Can we help prevent Alzheimer's with diet?, ...   \n",
      "3              ( Do any benefits of alcohol outweigh the risk...   \n",
      "4              ( Does acupuncture work for headaches?,  Does ...   \n",
      "\n",
      "                                                       instances title  \n",
      "fact_check_id                                                           \n",
      "0              [(1525653998.0, https://metafact.io/factchecks...        \n",
      "1              [(1617955634.0, https://metafact.io/factchecks...        \n",
      "2              [(1525653998.0, https://metafact.io/factchecks...        \n",
      "3              [(1525653998.0, https://metafact.io/factchecks...        \n",
      "4              [(1617955595.0, https://metafact.io/factchecks...        \n",
      "                                        instances  \\\n",
      "post_id                                             \n",
      "0                            [(1608571882.0, fb)]   \n",
      "1                            [(1586139153.0, fb)]   \n",
      "2        [(1610052141.0, fb), (1610072448.0, fb)]   \n",
      "3                            [(1645187790.0, ig)]   \n",
      "4                            [(1581697500.0, fb)]   \n",
      "\n",
      "                                                       ocr  \\\n",
      "post_id                                                      \n",
      "0        [(! Dreister Impf-Fake von Markus SÃ¶der! Es is...   \n",
      "1        [(!! WARNING !! A new thing circulating now. P...   \n",
      "2        [(\"Actually, he's a damn sight better than any...   \n",
      "3        [(\"Australia 50 MILLONES de dosis de \"vacuna\" ...   \n",
      "4        [(\"Bienaventurados los perseguidos por mi caus...   \n",
      "\n",
      "                    verdicts text  \n",
      "post_id                            \n",
      "0        [False information]       \n",
      "1        [False information]       \n",
      "2          [Missing context]       \n",
      "3                    [False]       \n",
      "4                         []       \n",
      "   post_id  fact_check_id\n",
      "0     2228             33\n",
      "1     2228          23568\n",
      "2     2228         194577\n",
      "3     2229             33\n",
      "4     2229          23568\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "from dataloader import get_data\n",
    "from utils import read_json\n",
    "from metrics import calculate_ndcg\n",
    "import torch\n",
    "\n",
    "from transformers import DPRConfig, DPRContextEncoder, DPRContextEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval\n",
    "import sys\n",
    "import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Check for Apple M2 GPU and set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use Metal Performance Shaders (MPS) for Apple GPUs\n",
    "    print(\"Using Apple M2 GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "batch_size = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e97f643a15f1b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:21:51.312338Z",
     "start_time": "2024-11-22T20:21:51.306437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove Jupyter's extra arguments\n",
    "argv = sys.argv\n",
    "if '-f' in argv:\n",
    "    argv = argv[:1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad922edcffa1e596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:22:02.541052Z",
     "start_time": "2024-11-22T20:22:02.534178Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--task TASK] [--lang LANG] [--split SPLIT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/anijasure_umass_edu/.local/share/jupyter/runtime/kernel-657734fb-a1ac-4869-a869-c0365fb477fc.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--task', type=str, default='monolingual', help='monolingual or crosslingual')\n",
    "parser.add_argument('--lang', type=str, default='eng', help='Language')\n",
    "parser.add_argument('--split', type=str, default='train', help='train, dev')\n",
    "args = parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5148b949829bf7ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:23:37.118438Z",
     "start_time": "2024-11-22T20:23:30.506125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: monolingual, Language: eng, Split: train\n",
      "Number of posts in train set: 4351\n",
      "Number of fact checks: 85734\n"
     ]
    }
   ],
   "source": [
    "TASK = \"monolingual\" #args.task\n",
    "LANG = \"eng\" # args.lang\n",
    "SPLIT = \"train\" # args.split\n",
    "\n",
    "print(f\"Task: {TASK}, Language: {LANG}, Split: {SPLIT}\")\n",
    "\n",
    "df_fact_checks, df_posts, df_fact_check_post_mapping = get_data('./data')\n",
    "tasks = read_json(f\"./data/tasks.json\")\n",
    "\n",
    "posts_split = tasks[TASK][LANG][f'posts_{SPLIT}']\n",
    "print(f\"Number of posts in {SPLIT} set:\", len(posts_split))\n",
    "\n",
    "fact_checks = tasks[TASK][LANG]['fact_checks']\n",
    "print(\"Number of fact checks:\", len(fact_checks))\n",
    "\n",
    "## filter dataframes\n",
    "df_posts_split = df_posts[df_posts.index.isin(posts_split)]\n",
    "assert len(df_posts_split) == len(posts_split)\n",
    "\n",
    "df_fact_checks = df_fact_checks[df_fact_checks.index.isin(fact_checks)]\n",
    "assert len(df_fact_checks) == len(fact_checks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b785ca810c0a88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:23:40.515614Z",
     "start_time": "2024-11-22T20:23:40.448715Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_681080/3682361778.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_posts_split['ocr_all_srclang'] = df_posts_split['ocr'].apply(lambda x: ' '.join([i[0] for i in x]) if x else \"\")\n",
      "/tmp/ipykernel_681080/3682361778.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_posts_split['text_srclang'] = df_posts_split['text'].apply(lambda x: x[0] if x else \"\")\n",
      "/tmp/ipykernel_681080/3682361778.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_posts_split['query'] = df_posts_split['ocr_all_srclang'] + ' ' + df_posts_split['text_srclang']\n"
     ]
    }
   ],
   "source": [
    "## DPR pre-processing\n",
    "\n",
    "# concat all OCR text from source language (0th index)\n",
    "df_posts_split['ocr_all_srclang'] = df_posts_split['ocr'].apply(lambda x: ' '.join([i[0] for i in x]) if x else \"\")\n",
    "\n",
    "# extract text from source language (0th index)\n",
    "df_posts_split['text_srclang'] = df_posts_split['text'].apply(lambda x: x[0] if x else \"\")\n",
    "\n",
    "# query: OCR + text -- is the Social Media Post SMP\n",
    "df_posts_split['query'] = df_posts_split['ocr_all_srclang'] + ' ' + df_posts_split['text_srclang']\n",
    "\n",
    "# extract claim and title from source language (0th index)\n",
    "df_fact_checks['claim_srclang'] = df_fact_checks['claim'].apply(lambda x: x[0] if x else \"\")\n",
    "df_fact_checks['title_srclang'] = df_fact_checks['title'].apply(lambda x: x[0] if x else \"\")\n",
    "\n",
    "# doc: claim + title --- FC doc\n",
    "df_fact_checks['doc'] = df_fact_checks['claim_srclang'] + ' ' + df_fact_checks['title_srclang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cafb70a0aff514f8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-22T20:45:29.865954Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## DPR\n",
    "\n",
    "# get doc embeddings i.e FC docs\n",
    "tokenizerDoc = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "modelDocEnc = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize documents\n",
    "tokenized_docs = tokenizerDoc(list(df_fact_checks['doc']), return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "# Create a dataset and data loader for batching\n",
    "dataset = TensorDataset(tokenized_docs[\"input_ids\"])\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa5c562-daf2-47a7-a541-d9de44ef912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_681080/3841170385.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddingsFCs = torch.load('/gypsum/work1/allan/anijasure/multilingual_semeval/IRMultiLingualSemEval/embeddingsFCs.pt')\n"
     ]
    }
   ],
   "source": [
    "# print(\"here\")\n",
    "# Compute embeddings in batches\n",
    "# embeddingsFCs_list = []\n",
    "# modelDocEnc.eval()  # Set the model to evaluation mode\n",
    "# with torch.no_grad():  # No need for gradients during inference\n",
    "#     for batch in tqdm(data_loader, desc=\"Processing batches\", total=len(data_loader)):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         input_ids_batch = batch[0]  # Extract the input IDs from the batch\n",
    "#         embeddings_batch = modelDocEnc(input_ids_batch).pooler_output\n",
    "#         embeddingsFCs_list.append(embeddings_batch.cpu())\n",
    "        \n",
    "\n",
    "# # Concatenate all embeddings into a single tensor\n",
    "# embeddingsFCs = torch.cat(embeddingsFCs_list, dim=0)\n",
    "\n",
    "# torch.save(embeddingsFCs, '/gypsum/work1/allan/anijasure/multilingual_semeval/IRMultiLingualSemEval/embeddingsFCs.pt')\n",
    "\n",
    "embeddingsFCs = torch.load('/gypsum/work1/allan/anijasure/multilingual_semeval/IRMultiLingualSemEval/embeddingsFCs.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a6b9892e685df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a FAISS index for document embeddings\n",
    "d = embeddingsFCs.shape[1]  # Dimensionality of embeddings\n",
    "nlist = 100  # Number of clusters (adjust based on data size)\n",
    "m = 32  # Number of connections per node in HNSW (controls trade-off between accuracy and speed)\n",
    "\n",
    "# quantizer = faiss.IndexFlatL2(d)  # L2 quantizer for clustering\n",
    "# index = faiss.IndexHNSWFlat(d, m)  # HNSW index with Flat quantizer\n",
    "# index.hnsw.efConstruction = 40  # Controls the quality of the graph during construction\n",
    "# index.hnsw.efSearch = 50  # Controls the number of neighbors explored during a search\n",
    "\n",
    "index = faiss.IndexFlatL2(d)  # Exact search using L2 distance\n",
    "\n",
    "\n",
    "# quantizer = faiss.IndexFlatIP(d)  # L2 quantizer for clustering\n",
    "# index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)  # HNSW index with Flat quantizer\n",
    "# index.nprobe = 10  # Controls the quality of the graph during construction\n",
    "# # index.hnsw.efSearch = 50  # Controls the number of neighbors explored during a search\n",
    "\n",
    "\n",
    "\n",
    "# Add document embeddings to the FAISS index\n",
    "index.add(embeddingsFCs)  # Add document embeddings to the FAISS index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d01b30b51f85eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Social Media Post embeddings    \n",
    "tokenizerSMP = tokenizerDoc # DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "modelSMP = modelDocEnc #  DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
    "\n",
    "# input_ids_SMP = tokenizerSMP(list(df_posts_split['query']), return_tensors=\"pt\", padding= True )[\"input_ids\"].to(device)\n",
    "# embeddingsSMP = modelSMP(input_ids_SMP).pooler_output.cpu()\n",
    "\n",
    "\n",
    "# Tokenize documents\n",
    "input_ids_SMP = tokenizerDoc(list(df_posts_split['query']), return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "# Create a dataset and data loader for batching\n",
    "dataset = TensorDataset(input_ids_SMP[\"input_ids\"])\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Compute embeddings in batches\n",
    "# embeddingsSMPs_list = []\n",
    "# modelSMP.eval()  # Set the model to evaluation mode\n",
    "# with torch.no_grad():  # No need for gradients during inference\n",
    "#     for batch in data_loader:\n",
    "#         input_ids_batch = batch[0]  # Extract the input IDs from the batch\n",
    "#         embeddings_batch = modelSMP(input_ids_batch).pooler_output\n",
    "#         embeddingsSMPs_list.append(embeddings_batch.cpu())\n",
    "\n",
    "# # Concatenate all embeddings into a single tensor\n",
    "# embeddingsSMP = torch.cat(embeddingsSMPs_list, dim=0)\n",
    "\n",
    "# torch.save(embeddingsSMP, '/gypsum/work1/allan/anijasure/multilingual_semeval/IRMultiLingualSemEval/embeddingsSMPs.pt')\n",
    "\n",
    "# embeddingsSMP = torch.load('/gypsum/work1/allan/anijasure/multilingual_semeval/IRMultiLingualSemEval/embeddingsSMPs.pt')\n",
    "\n",
    "\n",
    "# Retrieve the nearest fact-check documents for each post\n",
    "# k = 10  # Number of nearest neighbors\n",
    "# distances, indices = index.search(embeddingsSMP, k )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf3efd0bf4d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the nearest fact-check documents for each post\n",
    "k = 10  # Number of nearest neighbors\n",
    "distances, indices = index.search(embeddingsSMP, k )\n",
    "\n",
    "# Map results back to the original fact-check documents\n",
    "run = {}\n",
    "for i, query in enumerate(df_posts_split['query']):\n",
    "    matched_docs = [df_fact_checks.iloc[idx]['doc'] for idx in indices[i]]\n",
    "    run[query] = {\n",
    "        x: float(y) for x,y in zip( df_posts_split['matched_docs'], distances[i].tolist())\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15190a8557b221e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_check_ids = df_fact_checks.index.tolist()\n",
    "\n",
    "qrels = {}\n",
    "for idx, row in df_posts_split.iterrows():\n",
    "    query = row['query']\n",
    "\n",
    "    ground_truth = df_fact_check_post_mapping[\n",
    "        df_fact_check_post_mapping['post_id'] == idx\n",
    "        ]['fact_check_id'].tolist()\n",
    "    qrels[query] = {\n",
    "        x:1 for x in ground_truth\n",
    "    }\n",
    "    # print(\"Ground truth:\", ground_truth)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d34426855a2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'ndcg', 'map'})\n",
    "\n",
    "# Evaluate\n",
    "results = evaluator.evaluate(run)\n",
    "print(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d0bdf-ee58-4762-9a3d-dfa9c345cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_check_ids = df_fact_checks.index.tolist()\n",
    "    \n",
    "metrics=['P_3', 'P_5', 'P_10', 'map_cut_3', 'map_cut_5', 'map_cut_10', 'ndcg_cut_3', 'ndcg_cut_5', 'ndcg_cut_10']\n",
    "average_scores = {metric: 0.0 for metric in metrics}\n",
    "\n",
    "\n",
    "for idx, row in df_posts_split.iterrows():\n",
    "    qrels = {}\n",
    "    runs = {}\n",
    "\n",
    "    # Get the query text\n",
    "    query = row['query']\n",
    "\n",
    "    # Compute the embedding for the query\n",
    "    tokenized_query = tokenizerSMP(\n",
    "        query,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_embedding = modelSMP(tokenized_query[\"input_ids\"]).pooler_output.cpu().numpy()\n",
    "\n",
    "    # Search the FAISS index for the top 10 nearest neighbors\n",
    "    k = 10  # Number of top results\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    # Get the ranked fact-check IDs and scores\n",
    "    ranked_fact_checks = [fact_check_ids[i] for i in indices[0]]\n",
    "    ranked_fact_checks_scores = distances[0]\n",
    "\n",
    "    # Get ground truth fact-check IDs for the post\n",
    "    ground_truth = df_fact_check_post_mapping[\n",
    "        df_fact_check_post_mapping['post_id'] == idx\n",
    "    ]['fact_check_id'].tolist()\n",
    "\n",
    "    # Prepare QRELs and runs for evaluation\n",
    "    qrels[str(idx)] = {str(fc_id): 1 for fc_id in ground_truth}\n",
    "    runs[str(idx)] = {str(fc_id): float(score) for fc_id, score in zip(ranked_fact_checks, ranked_fact_checks_scores)}\n",
    "\n",
    "    # Evaluate using pytrec_eval\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, metrics)\n",
    "    results = evaluator.evaluate(runs)\n",
    "\n",
    "    for query_id in results:\n",
    "        for metric in metrics:\n",
    "            average_scores[metric] += results[query_id][metric]\n",
    "\n",
    "    \n",
    "for metric in average_scores:\n",
    "    average_scores[metric] /= len(df_posts_split)\n",
    "\n",
    "print(\"Average scores:\")\n",
    "print(average_scores)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
